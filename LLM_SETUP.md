# р╕Др╕╣р╣Ир╕бр╕╖р╕нр╕Бр╕▓р╕гр╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╣Бр╕ер╕░р╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ LLM (Ollama) р╕Ър╕Щр╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣М

## ЁЯУЛ р╕ар╕▓р╕Юр╕гр╕зр╕б

р╣Вр╕Ыр╕гр╣Ар╕Ир╕Бр╕Хр╣Мр╕Щр╕╡р╣Йр╣Гр╕Кр╣Й **Ollama** р╣Ар╕Ыр╣Зр╕Щ LLM (Large Language Model) р╕кр╕│р╕лр╕гр╕▒р╕Ър╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М network configuration р╣Вр╕Фр╕вр╕гр╕▒р╕Щ Ollama р╣Гр╕Щ Docker container р╣Ар╕Фр╕╡р╕вр╕зр╕Бр╕▒р╕Щр╕Бр╕▒р╕Ър╣Бр╕нр╕Ыр╕Юр╕ер╕┤р╣Ар╕Др╕Кр╕▒р╕Щ

## ЁЯПЧя╕П р╕кр╕Цр╕▓р╕Ыр╕▒р╕Хр╕вр╕Бр╕гр╕гр╕б

```
тФМтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФР
тФВ         Docker Network                  тФВ
тФВ                                         тФВ
тФВ  тФМтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФР      тФМтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФР        тФВ
тФВ  тФВ Backend  тФВтФАтФАтФАтФАтФАтФАтФВ  Ollama  тФВ        тФВ
тФВ  тФВ          тФВ      тФВ          тФВ        тФВ
тФВ  тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ      тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ        тФВ
тФВ       тФВ                  тФВ             тФВ
тФВ       тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ             тФВ
тФВ                                         тФВ
тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ
```

- **Backend** р╣Ар╕Кр╕╖р╣Ир╕нр╕бр╕Хр╣Ир╕нр╕Бр╕▒р╕Ъ **Ollama** р╕Ьр╣Ир╕▓р╕Щ Docker network (`http://ollama:11434`)
- р╕Чр╕▒р╣Йр╕Зр╕кр╕нр╕Зр╕гр╕▒р╕Щр╣Гр╕Щ Docker container р╣Ар╕Фр╕╡р╕вр╕зр╕Бр╕▒р╕Щ
- р╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕Бр╣Зр╕Ър╣Гр╕Щ Docker volume (`ollama-data`)

## ЁЯЪА р╕Бр╕▓р╕гр╕Хр╕┤р╕Фр╕Хр╕▒р╣Йр╕Зр╣Бр╕ер╕░р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓

### 1. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ Docker Compose Configuration

р╣Вр╕Ыр╕гр╣Ар╕Ир╕Бр╕Хр╣Мр╕бр╕╡ Ollama service р╕нр╕вр╕╣р╣Ир╣Гр╕Щ `docker-compose.yml` р╣Бр╕ер╕░ `docker-compose.prod.yml` р╣Бр╕ер╣Йр╕з:

```yaml
ollama:
  image: ollama/ollama:latest
  container_name: mnp-ollama-prod
  restart: unless-stopped
  ports:
    - "11434:11434"
  volumes:
    - ollama-data:/root/.ollama
  networks:
    - mnp-network
```

### 2. р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓ Environment Variables

р╕кр╕гр╣Йр╕▓р╕Зр╣Др╕Яр╕ер╣М `.env` р╣Гр╕Щр╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣М `backend/`:

```bash
cd backend
cp .env.example .env
nano .env
```

р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╕Фр╕▒р╕Зр╕Щр╕╡р╣Й:

```env
# AI Model Configuration (Ollama)
# р╕кр╕│р╕лр╕гр╕▒р╕Ъ Docker: р╣Гр╕Кр╣Й http://ollama:11434 (р╣Ар╕Кр╕╖р╣Ир╕нр╕бр╕Хр╣Ир╕нр╕Бр╕▒р╕Ъ Ollama container)
# Qwen2.5-coder:32b - р╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕Йр╕Юр╕▓р╕░р╕Чр╕▓р╕Зр╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ technical analysis (р╣Бр╕Щр╕░р╕Щр╕│)
AI_MODEL_NAME=qwen2.5-coder:32b
AI_MODEL_VERSION=v2-coder-32b
AI_MODEL_ENDPOINT=http://ollama:11434
```

**р╕кр╕│р╕Др╕▒р╕Н**: 
- р╕кр╕│р╕лр╕гр╕▒р╕Ъ **Production/Development р╣Гр╕Щ Docker**: р╣Гр╕Кр╣Й `http://ollama:11434`
- р╕кр╕│р╕лр╕гр╕▒р╕Ъ **Development р╕Ър╕Щ Host Machine**: р╣Гр╕Кр╣Й `http://host.docker.internal:11434` (р╕Цр╣Йр╕▓ Ollama р╕гр╕▒р╕Щр╕Ър╕Щ host)

### 3. р╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕е LLM

р╕лр╕ер╕▒р╕Зр╕Ир╕▓р╕Б start services р╣Бр╕ер╣Йр╕з р╣Гр╕лр╣Йр╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕е:

```bash
# р╣Ар╕Вр╣Йр╕▓р╣Др╕Ыр╣Гр╕Щ Ollama container
docker exec -it mnp-ollama-prod bash

# р╕лр╕гр╕╖р╕нр╣Гр╕Кр╣Йр╕Др╕│р╕кр╕▒р╣Ир╕Зр╕Щр╕╡р╣Йр╣Вр╕Фр╕вр╕Хр╕гр╕З
docker exec -it mnp-ollama-prod ollama pull qwen2.5-coder:32b
```

**р╕лр╕бр╕▓р╕вр╣Ар╕лр╕Хр╕╕**: р╕Бр╕▓р╕гр╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╕Др╕гр╕▒р╣Йр╕Зр╣Бр╕гр╕Бр╕нр╕▓р╕Ир╣Гр╕Кр╣Йр╣Ар╕зр╕ер╕▓р╕Щр╕▓р╕Щ (р╕Вр╕╢р╣Йр╕Щр╕нр╕вр╕╣р╣Ир╕Бр╕▒р╕Ър╕Вр╕Щр╕▓р╕Фр╣Вр╕бр╣Ар╕Фр╕ер╣Бр╕ер╕░р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕нр╕┤р╕Щр╣Ар╕Чр╕нр╕гр╣Мр╣Ар╕Щр╣Зр╕Х)

### 4. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕Бр╕▓р╕гр╕Чр╕│р╕Зр╕▓р╕Щ

```bash
# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓ Ollama container р╕Чр╕│р╕Зр╕▓р╕Щ
docker ps | grep ollama

# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ logs
docker logs mnp-ollama-prod

# р╕Чр╕Фр╕кр╕нр╕Ър╕Бр╕▓р╕гр╣Ар╕Кр╕╖р╣Ир╕нр╕бр╕Хр╣Ир╕нр╕Ир╕▓р╕Б Backend container
docker exec -it mnp-backend-prod curl http://ollama:11434/api/tags

# р╕Чр╕Фр╕кр╕нр╕Ър╕Бр╕▓р╕гр╣Ар╕гр╕╡р╕вр╕Бр╣Гр╕Кр╣Йр╣Вр╕бр╣Ар╕Фр╕е
docker exec -it mnp-ollama-prod ollama run qwen2.5-coder:32b "Hello, how are you?"
```

## ЁЯУЭ р╕Бр╕▓р╕гр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ

### р╣Ар╕гр╕┤р╣Ир╕бр╕Хр╣Йр╕Щ Services

```bash
# Development
docker-compose up -d

# Production
docker-compose -f docker-compose.prod.yml up -d
```

### р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕кр╕Цр╕▓р╕Щр╕░ Ollama

```bash
# р╕Фр╕╣ logs р╣Бр╕Ър╕Ъ real-time
docker logs -f mnp-ollama-prod

# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓р╣Вр╕бр╣Ар╕Фр╕ер╕Юр╕гр╣Йр╕нр╕бр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ
curl http://localhost:11434/api/tags
```

### р╣Ар╕Ыр╕ер╕╡р╣Ир╕вр╕Щр╣Вр╕бр╣Ар╕Фр╕е LLM

1. р╣Бр╕Бр╣Йр╣Др╕В `.env`:
   ```env
   AI_MODEL_NAME=llama3:8b  # р╣Ар╕Ыр╕ер╕╡р╣Ир╕вр╕Щр╣Ар╕Ыр╣Зр╕Щр╣Вр╕бр╣Ар╕Фр╕ер╕Чр╕╡р╣Ир╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕г
   ```

2. р╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╣Гр╕лр╕бр╣И:
   ```bash
   docker exec -it mnp-ollama-prod ollama pull llama3:8b
   ```

3. Restart backend:
   ```bash
   docker-compose restart backend
   ```

## ЁЯФз р╕Бр╕▓р╕гр╣Бр╕Бр╣Йр╣Др╕Вр╕Ыр╕▒р╕Нр╕лр╕▓

### р╕Ыр╕▒р╕Нр╕лр╕▓: Backend р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Ар╕Кр╕╖р╣Ир╕нр╕бр╕Хр╣Ир╕нр╕Бр╕▒р╕Ъ Ollama р╣Др╕Фр╣Й

**р╕нр╕▓р╕Бр╕▓р╕г**: Error message `[ERROR] Ollama call failed: ...`

**р╕зр╕┤р╕Шр╕╡р╣Бр╕Бр╣Йр╣Др╕В**:

1. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓ Ollama container р╕Чр╕│р╕Зр╕▓р╕Щ:
   ```bash
   docker ps | grep ollama
   ```

2. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ network connectivity:
   ```bash
   docker exec -it mnp-backend-prod ping -c 3 ollama
   ```

3. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ endpoint р╣Гр╕Щ `.env`:
   ```bash
   docker exec -it mnp-backend-prod env | grep AI_MODEL_ENDPOINT
   ```
   р╕Др╕зр╕гр╣Бр╕кр╕Фр╕З: `AI_MODEL_ENDPOINT=http://ollama:11434`

4. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ logs:
   ```bash
   docker logs mnp-backend-prod | grep -i ollama
   docker logs mnp-ollama-prod
   ```

### р╕Ыр╕▒р╕Нр╕лр╕▓: р╣Вр╕бр╣Ар╕Фр╕ер╕вр╕▒р╕Зр╣Др╕бр╣Ир╕Цр╕╣р╕Бр╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Ф

**р╕нр╕▓р╕Бр╕▓р╕г**: Error `model not found` р╕лр╕гр╕╖р╕н `404 Not Found`

**р╕зр╕┤р╕Шр╕╡р╣Бр╕Бр╣Йр╣Др╕В**:

```bash
# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╣Вр╕бр╣Ар╕Фр╕ер╕Чр╕╡р╣Ир╕бр╕╡р╕нр╕вр╕╣р╣И
docker exec -it mnp-ollama-prod ollama list

# р╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕е
docker exec -it mnp-ollama-prod ollama pull qwen2.5-coder:32b

# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕нр╕╡р╕Бр╕Др╕гр╕▒р╣Йр╕З
docker exec -it mnp-ollama-prod ollama list
```

### р╕Ыр╕▒р╕Нр╕лр╕▓: Ollama р╣Гр╕Кр╣Й RAM/CPU р╕бр╕▓р╕Бр╣Ар╕Бр╕┤р╕Щр╣Др╕Ы

**р╕зр╕┤р╕Шр╕╡р╣Бр╕Бр╣Йр╣Др╕В**:

1. р╣Гр╕Кр╣Йр╣Вр╕бр╣Ар╕Фр╕ер╕Вр╕Щр╕▓р╕Фр╣Ар╕ер╣Зр╕Бр╕Бр╕зр╣Ир╕▓:
   ```env
   AI_MODEL_NAME=qwen2.5-coder:14b  # р╣Бр╕Чр╕Щ 32b (р╕кр╕бр╕Фр╕╕р╕е)
   # р╕лр╕гр╕╖р╕н
   AI_MODEL_NAME=qwen2.5-coder:7b   # р╣Бр╕Чр╕Щ 32b (р╣Ар╕гр╣Зр╕зр╕Бр╕зр╣Ир╕▓)
   ```

2. р╕Ир╕│р╕Бр╕▒р╕Ф resources р╣Гр╕Щ docker-compose:
   ```yaml
   ollama:
     deploy:
       resources:
         limits:
           memory: 4G
           cpus: '2'
   ```

### р╕Ыр╕▒р╕Нр╕лр╕▓: Response р╕Кр╣Йр╕▓р╕бр╕▓р╕Бр╕лр╕гр╕╖р╕н Timeout

**р╕зр╕┤р╕Шр╕╡р╣Бр╕Бр╣Йр╣Др╕В**:

1. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ logs р╣Ар╕Юр╕╖р╣Ир╕нр╕Фр╕╣р╕зр╣Ир╕▓р╣Ар╕Бр╕┤р╕Фр╕нр╕░р╣Др╕гр╕Вр╕╢р╣Йр╕Щ:
   ```bash
   docker logs mnp-ollama-prod --tail 50
   ```

2. р╕ер╕Фр╕Вр╕Щр╕▓р╕Ф input (р╣Гр╕Щр╣Вр╕Др╣Йр╕Фр╕бр╕╡р╕Бр╕▓р╕гр╕Ир╕│р╕Бр╕▒р╕Ф original_content р╣Ар╕Ыр╣Зр╕Щ 5000 р╕Хр╕▒р╕зр╕нр╕▒р╕Бр╕йр╕гр╣Бр╕ер╣Йр╕з)

3. р╣Гр╕Кр╣Йр╣Вр╕бр╣Ар╕Фр╕ер╕Чр╕╡р╣Ир╣Ар╕гр╣Зр╕зр╕Бр╕зр╣Ир╕▓ (р╣Бр╕Хр╣Ир╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕нр╕▓р╕Ир╕ер╕Фр╕ер╕З)

4. р╣Ар╕Юр╕┤р╣Ир╕б timeout р╣Гр╕Щ `llm_service.py` (р╕Ыр╕▒р╕Ир╕Ир╕╕р╕Ър╕▒р╕Щ 600 р╕зр╕┤р╕Щр╕▓р╕Чр╕╡)

## ЁЯОп р╣Вр╕бр╣Ар╕Фр╕ер╕Чр╕╡р╣Ир╣Бр╕Щр╕░р╕Щр╕│р╕кр╕│р╕лр╕гр╕▒р╕Ъ Network Configuration Analysis

### Qwen2.5 Coder Models (р╣Бр╕Щр╕░р╕Щр╕│р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ Technical Analysis)

**Qwen2.5 Coder** р╣Ар╕Ыр╣Зр╕Щр╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕Йр╕Юр╕▓р╕░р╕Чр╕▓р╕Зр╕Чр╕╡р╣Ир╕нр╕нр╕Бр╣Бр╕Ър╕Ър╕бр╕▓р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ technical analysis, code generation, р╣Бр╕ер╕░ code reasoning р╣Вр╕Фр╕вр╣Ар╕Йр╕Юр╕▓р╕░ р╣Ар╕лр╕бр╕▓р╕░р╕Бр╕▒р╕Ър╕Зр╕▓р╕Щ Network Configuration Analysis р╕бр╕▓р╕Бр╕Чр╕╡р╣Ир╕кр╕╕р╕Ф

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM 32GB+

- `qwen2.5-coder:32b` тнР (р╣Бр╕Щр╕░р╕Щр╕│р╕кр╕╣р╕Зр╕кр╕╕р╕Ф) - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕кр╕╣р╕Зр╕кр╕╕р╕Ф, р╣Ар╕Чр╕╡р╕вр╕Ър╣Ар╕Чр╣Ир╕▓ GPT-4o р╣Гр╕Щр╕Зр╕▓р╕Щ technical
  - RAM: ~16-20GB
  - Model size: ~18GB
  - р╣Ар╕лр╕бр╕▓р╕░р╕Бр╕▒р╕Ър╕Зр╕▓р╕Щ Network Configuration Analysis р╕Чр╕╡р╣Ир╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕кр╕╣р╕Зр╕кр╕╕р╕Ф
  - р╕гр╕нр╕Зр╕гр╕▒р╕Ъ structured output (JSON) р╣Др╕Фр╣Йр╕Фр╕╡р╕бр╕▓р╕Б

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM 16-32GB

- `qwen2.5-coder:14b` - р╕кр╕бр╕Фр╕╕р╕ер╕гр╕░р╕лр╕зр╣Ир╕▓р╕Зр╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╣Бр╕ер╕░р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕з
  - RAM: ~8-10GB
  - Model size: ~8GB
  - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Фр╕╡р╕бр╕▓р╕Бр╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ technical

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM 8-16GB

- `qwen2.5-coder:7b` - р╣Ар╕гр╣Зр╕зр╕Бр╕зр╣Ир╕▓р╣Бр╕Хр╣Ир╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Хр╣Ир╕│р╕Бр╕зр╣Ир╕▓
  - RAM: ~4-6GB
  - Model size: ~4GB
  - р╕вр╕▒р╕Зр╕Др╕Зр╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Фр╕╡р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ technical

### Qwen2.5 General Models (р╕Чр╕▓р╕Зр╣Ар╕ер╕╖р╕нр╕Б)

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM 8GB+

- `qwen2.5:7b` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Фр╕╡, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Ыр╕▓р╕Щр╕Бр╕ер╕▓р╕З
- `llama3:8b` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Фр╕╡р╕бр╕▓р╕Б, р╣Гр╕Кр╣Й RAM р╕бр╕▓р╕Б
- `mistral:7b` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Фр╕╡, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Фр╕╡

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM 4-8GB

- `qwen2.5:3b` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Ыр╕▓р╕Щр╕Бр╕ер╕▓р╕З, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Фр╕╡
- `llama3:3b` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Ыр╕▓р╕Щр╕Бр╕ер╕▓р╕З, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Фр╕╡
- `phi3:mini` - р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Ыр╕▓р╕Щр╕Бр╕ер╕▓р╕З, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Фр╕╡р╕бр╕▓р╕Б

#### р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Ар╕Лр╕┤р╕гр╣Мр╕Яр╣Ар╕зр╕нр╕гр╣Мр╕Чр╕╡р╣Ир╕бр╕╡ RAM р╕Щр╣Йр╕нр╕вр╕Бр╕зр╣Ир╕▓ 4GB

- `phi3:mini` - р╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕ер╣Зр╕Б, р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕зр╕Фр╕╡
- `tinyllama` - р╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕ер╣Зр╕Бр╕бр╕▓р╕Б, р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╕Хр╣Ир╕│

### р╕Чр╕│р╣Др╕бр╕Хр╣Йр╕нр╕Зр╣Гр╕Кр╣Й Qwen2.5 Coder?

1. **Specialized Training**: р╕Цр╕╣р╕Б train р╣Ар╕Йр╕Юр╕▓р╕░р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Зр╕▓р╕Щ technical/code analysis
2. **Superior Performance**: р╕бр╕╡р╕Ыр╕гр╕░р╕кр╕┤р╕Чр╕Шр╕┤р╕ар╕▓р╕Юр╣Ар╕Чр╕╡р╕вр╕Ър╣Ар╕Чр╣Ир╕▓ GPT-4o р╣Гр╕Щр╕Зр╕▓р╕Щ technical benchmarks
3. **Network Configuration Fit**: р╣Ар╕Вр╣Йр╕▓р╣Гр╕И configuration syntax, command structures р╣Др╕Фр╣Йр╕Фр╕╡р╕Бр╕зр╣Ир╕▓
4. **Structured Output**: р╕кр╕гр╣Йр╕▓р╕З structured JSON outputs р╣Др╕Фр╣Йр╕Фр╕╡р╕бр╕▓р╕Б
5. **Technical Reasoning**: р╕бр╕╡р╕Др╕зр╕▓р╕бр╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Гр╕Щр╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Бр╕ер╕░р╕нр╕Шр╕┤р╕Ър╕▓р╕в technical concepts р╣Др╕Фр╣Йр╕Фр╕╡

## ЁЯУК р╕Бр╕▓р╕гр╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ Performance

### р╕Фр╕╣ Token Usage р╣Бр╕ер╕░ Latency

р╕Фр╕╣р╣Др╕Фр╣Йр╕Ир╕▓р╕Б API response р╣Ар╕бр╕╖р╣Ир╕нр╕кр╕гр╣Йр╕▓р╕З analysis:

```json
{
  "llm_metrics": {
    "inference_time_ms": 1234.5,
    "token_usage": {
      "prompt_tokens": 500,
      "completion_tokens": 300,
      "total_tokens": 800
    },
    "model_name": "qwen2.5-coder:32b"
  }
}
```

### р╕Фр╕╣ Performance Metrics р╕Ир╕▓р╕Б Database

```bash
# р╣Ар╕Вр╣Йр╕▓ MongoDB
docker exec -it mnp-mongo-prod mongo

# р╕Фр╕╣ performance logs
use manage_network_projects
db.performance_logs.find().sort({timestamp: -1}).limit(10).pretty()
```

## ЁЯФТ Security Considerations

1. **р╣Др╕бр╣Ир╣Ар╕Ыр╕┤р╕Фр╣Ар╕Ьр╕в Ollama port р╣Др╕Ыр╕вр╕▒р╕З public** (р╕Цр╣Йр╕▓р╣Др╕бр╣Ир╕Ир╕│р╣Ар╕Ыр╣Зр╕Щ):
   - Port 11434 р╣Др╕бр╣Ир╕Др╕зр╕г expose р╣Др╕Ыр╕вр╕▒р╕З public network
   - р╣Гр╕Кр╣Йр╕Ьр╣Ир╕▓р╕Щ Docker network р╣Ар╕Чр╣Ир╕▓р╕Щр╕▒р╣Йр╕Щ

2. **Backup р╣Вр╕бр╣Ар╕Фр╕е** (р╕Цр╣Йр╕▓р╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕г):
   ```bash
   # Backup volume
   docker run --rm -v ollama-data:/data -v $(pwd):/backup alpine tar czf /backup/ollama-backup.tar.gz /data
   ```

3. **Monitor Resource Usage**:
   ```bash
   docker stats mnp-ollama-prod
   ```

## ЁЯУЪ р╣Ар╕нр╕Бр╕кр╕▓р╕гр╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕б

- [Ollama Official Documentation](https://ollama.ai/docs)
- [ANALYSIS_SYSTEM.md](backend/ANALYSIS_SYSTEM.md) - р╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╕гр╕░р╕Ър╕Ъ Analysis
- [README.md](README.md) - р╕Др╕╣р╣Ир╕бр╕╖р╕нр╕лр╕ер╕▒р╕Бр╕Вр╕нр╕Зр╣Вр╕Ыр╕гр╣Ар╕Ир╕Бр╕Хр╣М

## ЁЯЖШ Support

р╕лр╕▓р╕Бр╕Юр╕Ър╕Ыр╕▒р╕Нр╕лр╕▓р╕лр╕гр╕╖р╕нр╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕Др╕зр╕▓р╕бр╕Кр╣Ир╕зр╕вр╣Ар╕лр╕ер╕╖р╕н:

1. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ logs: `docker logs mnp-ollama-prod`
2. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ network: `docker network inspect mnp-network`
3. р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ environment variables: `docker exec mnp-backend-prod env | grep AI`
4. р╕Фр╕╣р╣Ар╕нр╕Бр╕кр╕▓р╕г Ollama: https://ollama.ai/docs
