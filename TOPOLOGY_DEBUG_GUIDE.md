# Topology LLM Debug Guide

## ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM Topology Generation ‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM Connection (‡∏ú‡πà‡∏≤‡∏ô API)

1. **Login ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö Token:**
   ```bash
   curl -X POST "http://10.4.15.167:8000/auth/login" \
     -H "Content-Type: application/json" \
     -d '{"username": "admin", "password": "admin123"}'
   ```
   
   ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å `access_token` ‡∏à‡∏≤‡∏Å response

2. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM Connection:**
   ```bash
   curl -X POST "http://10.4.15.167:8000/topology/test-llm" \
     -H "Authorization: Bearer YOUR_TOKEN_HERE" \
     -H "Content-Type: application/json"
   ```

   Endpoint ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö:
   - ‚úÖ Ollama connectivity
   - ‚úÖ Model availability (`qwen2.5-coder:14b`)
   - ‚úÖ Simple LLM call
   - ‚úÖ Topology LLM with sample data

3. **‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
   - `ollama_accessible`: true/false
   - `model_available`: true/false
   - `simple_call_works`: true/false
   - `topology_call_works`: true/false
   - `errors`: [] (‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ errors ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Generate Topology (‡∏à‡∏£‡∏¥‡∏á)

1. **Generate Topology ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Project:**
   ```bash
   curl -X POST "http://10.4.15.167:8000/projects/YOUR_PROJECT_ID/topology/generate" \
     -H "Authorization: Bearer YOUR_TOKEN_HERE" \
     -H "Content-Type: application/json"
   ```

2. **‡∏î‡∏π Logs ‡∏à‡∏≤‡∏Å Backend Container:**
   ```bash
   sudo docker logs backend --tail 100 | grep -E "Topology|Ollama|ERROR|Error"
   ```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤

#### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: `ollama_accessible: false`
**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** Ollama container ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama container ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
sudo docker ps | grep ollama

# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÉ‡∏´‡πâ start
sudo docker-compose up -d ollama

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs
sudo docker logs ollama --tail 50
```

#### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: `model_available: false`
**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** Model `qwen2.5-coder:14b` ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ pull

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# Pull model
sudo docker exec ollama ollama pull qwen2.5-coder:14b

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ pull ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
sudo docker exec ollama ollama list
```

#### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: `simple_call_works: false` ‡∏´‡∏£‡∏∑‡∏≠ "Simple LLM call timeout"
**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** Ollama ‡∏°‡∏µ **server-side timeout ~60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ** (hardcoded) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 60s ‡∏à‡∏∞‡πÑ‡∏î‡πâ HTTP 500

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥):**
1. **‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• 14b ‡πÅ‡∏ó‡∏ô 32b** ‚Äî 14b ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 60s ‡∏ö‡∏ô CPU  
   ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö `backend/.env`:
   ```bash
   grep AI_MODEL_NAME backend/.env
   ```
   ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô `AI_MODEL_NAME=qwen2.5-coder:14b` (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 32b)

2. **Pre-load ‡πÇ‡∏°‡πÄ‡∏î‡∏• 14b** ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏≠ load ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô 60s):
   ```bash
   sudo docker exec mnp-ollama ollama run qwen2.5-coder:14b "hello"
   ```
   ‡∏£‡∏≠‡∏à‡∏ô‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏¢‡∏¥‡∏á test-llm ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

3. **Restart backend** ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ .env:
   ```bash
   sudo docker compose restart backend
   ```

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:**
- ‡∏î‡∏π `errors` array ‡πÉ‡∏ô response
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs: `sudo docker logs mnp-backend --tail 50`
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö resources: `sudo docker stats mnp-ollama`

#### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: `topology_call_works: false`
**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** LLM ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parse JSON ‡∏´‡∏£‡∏∑‡∏≠ return structure ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
- ‡∏î‡∏π `errors` array ‡πÅ‡∏•‡∏∞ `raw_response` ‡πÉ‡∏ô response
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ LLM return JSON format ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô Swagger UI

1. ‡πÄ‡∏õ‡∏¥‡∏î Swagger UI: `http://10.4.15.167:8000/docs`

2. **Authorize:**
   - ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° "Authorize" (üîí)
   - ‡πÉ‡∏™‡πà token (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà "Bearer " ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)
   - ‡∏Ñ‡∏•‡∏¥‡∏Å "Authorize" ‡πÅ‡∏•‡πâ‡∏ß "Close"

3. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö `/topology/test-llm`:**
   - ‡∏´‡∏≤ endpoint `POST /topology/test-llm`
   - ‡∏Ñ‡∏•‡∏¥‡∏Å "Try it out"
   - ‡∏Ñ‡∏•‡∏¥‡∏Å "Execute"
   - ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

4. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö `/projects/{project_id}/topology/generate`:**
   - ‡∏´‡∏≤ endpoint `POST /projects/{project_id}/topology/generate`
   - ‡πÉ‡∏™‡πà `project_id` ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
   - ‡∏Ñ‡∏•‡∏¥‡∏Å "Try it out" ‡πÅ‡∏•‡πâ‡∏ß "Execute"
   - ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞ errors (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Backend Logs

```bash
# ‡∏î‡∏π logs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
sudo docker logs backend --tail 200

# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Topology-related logs
sudo docker logs backend --tail 200 | grep -i topology

# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Ollama-related logs
sudo docker logs backend --tail 200 | grep -i ollama

# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Errors
sudo docker logs backend --tail 200 | grep -i error
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama Logs

```bash
# ‡∏î‡∏π logs ‡∏Ç‡∏≠‡∏á Ollama
sudo docker logs ollama --tail 100

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model ‡∏ñ‡∏π‡∏Å load ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
sudo docker exec ollama ollama list

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ollama ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
sudo docker exec ollama ollama run qwen2.5-coder:14b "Say hello"
```

## ‡∏™‡∏£‡∏∏‡∏õ Checklist

- [ ] Ollama container ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà
- [ ] Model `qwen2.5-coder:14b` ‡∏ñ‡∏π‡∏Å pull ‡πÅ‡∏•‡πâ‡∏ß
- [ ] `/topology/test-llm` endpoint ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
- [ ] `ollama_accessible: true`
- [ ] `model_available: true`
- [ ] `simple_call_works: true`
- [ ] `topology_call_works: true`
- [ ] `/projects/{project_id}/topology/generate` ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
- [ ] Topology result ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB

## Troubleshooting Tips

1. **‡∏ñ‡πâ‡∏≤ timeout:** ‡∏•‡∏î `num_predict` ‡πÉ‡∏ô `topology_service.py` ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° timeout
2. **‡∏ñ‡πâ‡∏≤ JSON parse error:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ LLM return format ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏î‡∏π `raw_response`)
3. **‡∏ñ‡πâ‡∏≤ connection error:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö network connectivity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á backend ‡πÅ‡∏•‡∏∞ ollama containers
4. **‡∏ñ‡πâ‡∏≤ model not found:** Pull model ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á: `sudo docker exec ollama ollama pull qwen2.5-coder:14b`

## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

- Backend logs: `sudo docker logs backend`
- Ollama logs: `sudo docker logs ollama`
- MongoDB: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö `parsed_configs` collection ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• devices ‡πÅ‡∏•‡∏∞ neighbors ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
